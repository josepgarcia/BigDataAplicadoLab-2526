# Configuración de Spark

# Configuración del Master
spark.master                     spark://master:7077
spark.eventLog.enabled           true
spark.eventLog.dir               file:///opt/spark/logs
spark.history.fs.logDirectory    file:///opt/spark/logs
spark.io.compression.codec       snappy

# Configuración de memoria
#spark.driver.memory              1g
#spark.executor.memory            2g
#spark.executor.cores             2
spark.driver.memory              512m
spark.executor.memory            512m
spark.executor.cores             1

# Sistema de ficheros por defecto (HDFS en este caso, ya que tenemos Hadoop)
# En modulo2/Spark/config/spark-defaults.conf era file:/// porque era standalone sin HDFS por defecto?
# No, en modulo2 tenemos HDFS. Pero Spark puede usar local o HDFS.
# Vamos a usar HDFS como defaultFS para que sea más integrado.
# Pero espera, en modulo2/Spark/start-spark.sh se configura fs.defaultFS a hdfs://master:9000
# Aquí en modulo2, core-site.xml ya apunta a hdfs://master:9000 (verificaremos).
# Si core-site.xml está bien, spark.hadoop.fs.defaultFS no es estrictamente necesario si lee core-site.xml.
# Pero para asegurar, podemos poner hdfs://master:9000 o dejar que lo coja de core-site.xml.
# En modulo2/Spark/config/spark-defaults.conf ponía file:///.
# Vamos a poner hdfs://master:9000 para aprovechar HDFS.
spark.hadoop.fs.defaultFS        hdfs://master:9000

# Configuración de UI
spark.ui.port                    4040
spark.ui.enabled                 true

# Serialización
spark.serializer                 org.apache.spark.serializer.KryoSerializer

# Configuración de Python
spark.pyspark.python             python3
spark.pyspark.driver.python      python3

# Logs
spark.executor.logs.rolling.strategy           time
spark.executor.logs.rolling.time.interval      daily
spark.executor.logs.rolling.maxRetainedFiles   7
